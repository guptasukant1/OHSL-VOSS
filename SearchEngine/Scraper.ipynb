{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "def save_image(image_url, folder):\n",
    "    try:\n",
    "        img_response = requests.get(image_url)\n",
    "        if img_response.status_code == 200:\n",
    "            img_name = os.path.basename(urlparse(image_url).path)\n",
    "            img_path = os.path.join(folder, img_name)\n",
    "            with open(img_path, 'wb') as img_file:\n",
    "                img_file.write(img_response.content)\n",
    "            return img_name\n",
    "        else:\n",
    "            print(f\"Failed to retrieve image. Status code: {img_response.status_code}\")\n",
    "            return None \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while saving image: {e}\")\n",
    "        return None\n",
    "\n",
    "def scrape_project(url, download_folder='downloaded_images'):\n",
    "    try:\n",
    "        if not os.path.exists(download_folder):\n",
    "            os.makedirs(download_folder)\n",
    "\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            def print_element(element, level=0):\n",
    "                indent = '    ' * level\n",
    "                if element.name == 'h1':\n",
    "                    print(f\"\\n# {element.get_text()}\\n\")\n",
    "                elif element.name == 'h2':\n",
    "                    print(f\"\\n## {element.get_text()}\\n\")\n",
    "                elif element.name == 'h3':\n",
    "                    print(f\"\\n### {element.get_text()}\\n\")\n",
    "                elif element.name == 'p':\n",
    "                    print(f\"{indent}{element.get_text()}\\n\")\n",
    "                elif element.name == 'ul':\n",
    "                    for li in element.find_all('li'):\n",
    "                        print(f\"{indent} - {li.get_text()}\")\n",
    "                elif element.name == 'img':\n",
    "                    img_url = urljoin(url, element.get('src'))\n",
    "                    img_name = save_image(img_url, download_folder)\n",
    "                    if img_name:\n",
    "                        img_path = os.path.join(download_folder, img_name)\n",
    "                        print(f\"{indent}![{element.get('alt', 'Image')}]({img_path})\")\n",
    "\n",
    "            nav = soup.find('nav')\n",
    "            if nav:\n",
    "                print(\"\\nNavigation Bar:\\n\")\n",
    "                for li in nav.find_all('li'):\n",
    "                    print(f\" - {li.get_text()}\")\n",
    "                print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "  \n",
    "            main_content = soup.find('div', {'class': 'main-content'})\n",
    "            if not main_content:\n",
    "                main_content = soup.find('div', {'class': 'content'})\n",
    "\n",
    "            if main_content:\n",
    "                for element in main_content.find_all(['h1', 'h2', 'h3', 'p', 'ul', 'img']):\n",
    "                    print_element(element)\n",
    "            else:\n",
    "                print(\"Main content not found. Please check the class name used to identify the main content.\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the website. Status code: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "project_url = input(\"Enter the URL of the project page: \").strip()\n",
    "\n",
    "if project_url.startswith('http://') or project_url.startswith('https://'):\n",
    "    scrape_project(project_url)\n",
    "else:\n",
    "    print(\"Invalid URL. Please ensure the URL starts with http:// or https://\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Navigation Bar:\n",
      "\n",
      " - \n",
      "Log in\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "# Projects\n",
      "\n",
      ": The goal of the ICTBiomed project is to create an informatics platform leveraging the joint capacity of global research community in order to provide an environment for comprehensive cancer research based on tools that had emerged in scientific communities across the world.\n",
      "![](downloaded_images/ICTBioMed%20logo.png)\n",
      ": ICKA’s mission is to enable and facilitate the generation, use and reuse of knowledge in India via the country’s own institutions, people, technology and data; and to address dimensions of the Indian cancer problem in a way that is comprehensive, scalable, sustainable and affordable.\n",
      "![](downloaded_images/icka%20logo_0.png)\n",
      ": The Mycroft Cognitive Assistant® (Mycroft) is designed to provide research universities and medical centers with support and assistance in applying for research grants from the U.S. National Institutes of Health (NIH).\n",
      "![](downloaded_images/Logo-Mycroft-Extended-250_0.png)\n",
      ": Breast cancer (BC) is the most common incident site of cancer in women worldwide accounting for 24.5 % of all cancers ( Globocan 2020).  Amongst the new cases of breast cancer in 2020, Asia has the largest incidence accounting for 45.4% amongst females of all ages. Interestingly, new breast cancer incidence cases amongst females under 45 years of age also has the largest percentage in Asia (52.9%). Furthermore, mortality due to breast cancer in this age group also peaks in Asia with 51.4 %. Five year prevalence in breast cancer in the same age group also has Asia peaking at 51.1%. In the United States too breast is the most common site for cancer amongst females of all ages ( 39.9%) with the percentage jumping to 45.6 % amongst females under 50 years of age.\n",
      ": To develop a clinical trial for the Oral Cancer Global Network (OCGN) focusing on early diagnosis of potentially malignant /high risk oral mucosal lesions (WHO definition: SIN III) and malignant lesions, including the establishment of the research team,\n",
      "the development of tools for data management and oversight of the research, the development of a trial design and other essential elements of the study, such as the protocol, recruitment strategies, and procedure manuals; and to collect feasibility data.\n",
      "![](downloaded_images/ocgn-05-768x325.png)\n",
      ": ​TELESYNERGY® is the use of equipment and communication devices for long-distance collaboration between medical institutions. TELESYNERGY® can be used for any activity that relates to the education, prevention, detection, diagnosis, treatment, and follow-up of any patient’s condition in any medical field. Developed by National Institutes of Health scientists and engineers to facilitate communications between cancer researchers around the world.\n",
      "![](downloaded_images/TELESYNERGY%C2%AE.jpg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "download_folder = 'downloaded_images/'\n",
    "\n",
    "def save_image(image_url, folder):\n",
    "    try:\n",
    "        img_response = requests.get(image_url)\n",
    "        if img_response.status_code == 200:\n",
    "            img_name = os.path.basename(urlparse(image_url).path)\n",
    "            img_path = os.path.join(folder, img_name)\n",
    "            with open(img_path, 'wb') as img_file:\n",
    "                img_file.write(img_response.content)\n",
    "            return img_name\n",
    "        else:\n",
    "            print(f\"Failed to retrieve image. Status code: {img_response.status_code}\")\n",
    "            return None \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while saving image: {e}\")\n",
    "        return None\n",
    "\n",
    "def print_element(element, url, level=0):\n",
    "    indent = '    ' * level\n",
    "    if element.name == 'h1':\n",
    "        print(f\"\\n# {element.get_text()}\\n\")\n",
    "    elif element.name == 'h2':\n",
    "        print(f\"\\n## {element.get_text()}\\n\")\n",
    "    elif element.name == 'h3':\n",
    "        print(f\"\\n### {element.get_text()}\\n\")\n",
    "    elif element.name == 'p':\n",
    "        print(f\"{indent}{element.get_text()}\\n\")\n",
    "    elif element.name == 'ul':\n",
    "        for li in element.find_all('li'):\n",
    "            print(f\"{indent} - {li.get_text()}\")\n",
    "    elif element.name == 'img':\n",
    "        img_url = urljoin(url, element.get('src'))\n",
    "        img_name = save_image(img_url, download_folder)\n",
    "        if img_name:\n",
    "            img_path = os.path.join(download_folder, img_name)\n",
    "            print(f\"{indent}![{element.get('alt', 'Image')}]({img_path})\")\n",
    "\n",
    "def scrape_project(url, download_folder='downloaded_images'):\n",
    "    try:\n",
    "        if not os.path.exists(download_folder):\n",
    "            os.makedirs(download_folder)\n",
    "\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            nav = soup.find('nav')\n",
    "            if nav:\n",
    "                print(\"\\nNavigation Bar:\\n\")\n",
    "                for li in nav.find_all('li'):\n",
    "                    print(f\" - {li.get_text()}\")\n",
    "                print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "            main_content = soup.find('div', {'class': 'main-content'})\n",
    "            if not main_content:\n",
    "                main_content = soup.find('div', {'class': 'content'})\n",
    "\n",
    "            if main_content:\n",
    "                for element in main_content.find_all(['h1', 'h2', 'h3', 'p', 'ul', 'img', 'table']):\n",
    "                    if element.name == 'table':\n",
    "                        for row in element.find_all('tr'):\n",
    "                            cols = row.find_all('td')\n",
    "                            if len(cols) >= 2:\n",
    "                                first_td = cols[0].get_text(strip=True)\n",
    "                                second_td = cols[1].get_text(strip=True)\n",
    "                                print(f\"{first_td}: {second_td}\")\n",
    "                    else:\n",
    "                        print_element(element, url)\n",
    "            else:\n",
    "                print(\"Main content not found. Please check the class name used to identify the main content.\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the website. Status code: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "project_url = input(\"Enter the URL of the project page: \").strip()\n",
    "\n",
    "if project_url.startswith('http://') or project_url.startswith('https://'):\n",
    "    scrape_project(project_url)\n",
    "else:\n",
    "    print(\"Invalid URL. Please ensure the URL starts with http:// or https://\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
