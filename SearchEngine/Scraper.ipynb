{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "def save_image(image_url, folder):\n",
    "    try:\n",
    "        img_response = requests.get(image_url)\n",
    "        if img_response.status_code == 200:\n",
    "            img_name = os.path.basename(urlparse(image_url).path)\n",
    "            img_path = os.path.join(folder, img_name)\n",
    "            with open(img_path, 'wb') as img_file:\n",
    "                img_file.write(img_response.content)\n",
    "            return img_name\n",
    "        else:\n",
    "            print(f\"Failed to retrieve image. Status code: {img_response.status_code}\")\n",
    "            return None \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while saving image: {e}\")\n",
    "        return None\n",
    "\n",
    "def scrape_project(url, download_folder='downloaded_images'):\n",
    "    try:\n",
    "        if not os.path.exists(download_folder):\n",
    "            os.makedirs(download_folder)\n",
    "\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            def print_element(element, level=0):\n",
    "                indent = '    ' * level\n",
    "                if element.name == 'h1':\n",
    "                    print(f\"\\n# {element.get_text()}\\n\")\n",
    "                elif element.name == 'h2':\n",
    "                    print(f\"\\n## {element.get_text()}\\n\")\n",
    "                elif element.name == 'h3':\n",
    "                    print(f\"\\n### {element.get_text()}\\n\")\n",
    "                elif element.name == 'p':\n",
    "                    print(f\"{indent}{element.get_text()}\\n\")\n",
    "                elif element.name == 'ul':\n",
    "                    for li in element.find_all('li'):\n",
    "                        print(f\"{indent} - {li.get_text()}\")\n",
    "                elif element.name == 'img':\n",
    "                    img_url = urljoin(url, element.get('src'))\n",
    "                    img_name = save_image(img_url, download_folder)\n",
    "                    if img_name:\n",
    "                        img_path = os.path.join(download_folder, img_name)\n",
    "                        print(f\"{indent}![{element.get('alt', 'Image')}]({img_path})\")\n",
    "\n",
    "            nav = soup.find('nav')\n",
    "            if nav:\n",
    "                print(\"\\nNavigation Bar:\\n\")\n",
    "                for li in nav.find_all('li'):\n",
    "                    print(f\" - {li.get_text()}\")\n",
    "                print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "  \n",
    "            main_content = soup.find('div', {'class': 'main-content'})\n",
    "            if not main_content:\n",
    "                main_content = soup.find('div', {'class': 'content'})\n",
    "\n",
    "            if main_content:\n",
    "                for element in main_content.find_all(['h1', 'h2', 'h3', 'p', 'ul', 'img']):\n",
    "                    print_element(element)\n",
    "            else:\n",
    "                print(\"Main content not found. Please check the class name used to identify the main content.\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the website. Status code: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "project_url = input(\"Enter the URL of the project page: \").strip()\n",
    "\n",
    "if project_url.startswith('http://') or project_url.startswith('https://'):\n",
    "    scrape_project(project_url)\n",
    "else:\n",
    "    print(\"Invalid URL. Please ensure the URL starts with http:// or https://\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Navigation Bar:\n",
      "\n",
      " - \n",
      "Log in\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "# OHSL Breast Cancer Data Alliance (BCDA)\n",
      "\n",
      "\n",
      "Breast cancer (BC) is the most common incident site of cancer in women worldwide accounting for 24.5 % of all cancers (Globocan 2020).  Amongst the new cases of breast cancer in 2020, Asia has the largest incidence accounting for 45.4% amongst females of all ages. Interestingly, new breast cancer incidence cases amongst females under 45 years of age also has the largest percentage in Asia (52.9%). Furthermore, mortality due to breast cancer in this age group also peaks in Asia with 51.4 %. Five year prevalence in breast cancer in the same age group also has Asia peaking at 51.1%. In the United States too breast is the most common site for cancer amongst females of all ages (39.9%) with the percentage jumping to 45.6 % amongst females under 50 years of age. [1]\n",
      "\n",
      "In the United States, during 2001-2015, incidence rates of early-onset metastatic breast cancer increased sharply among NH white, NH black, Hispanic, and Asian/Pacific Islander (API) women [2].\n",
      "\n",
      "Asia has 44% of the world’s BC deaths with 39% of overall new BC cases diagnosed. Amongst the Asian countries, approximately 25% of the female cancer cases in India are BC. The rate of incidence was found to be 25.8 in 100,000 women and the mortality rate is 12.7 per 100,000 women (2017). Another troubling concern about the scenario of BC in India is the increased incidence of the disease in younger Indian women (between the ages of 30 and 40). Presently, almost 48% of patients with BC in India are below 50 years of age. There is an increasing trend of BC in women between the ages of 25 and 40 in the past 25 years. The latest surveillance trends from 2000 to 2014 based on registries from 71 countries estimated the 5-year survival ratio to be 66.1% in India which is the lowest levels among the countries included in the study [3].\n",
      "\n",
      "Considering the fact that Breast cancer (BC) is a significant global health concern, with substantial variations in incidence and mortality rates across different regions, the OHSL Breast Cancer Data Alliance (BCDA) project aims to leverage the power of Big Data and precision medicine to better understand and address breast cancer, particularly focusing on risk factors and global trends.\n",
      "\n",
      "Advances in a wide array of scientific technologies have brought data of unprecedented volume and complexity into the oncology research space. These novel big data resources are applied across a variety of contexts-from health services research using data from insurance claims, cancer registries, and electronic health records, to deeper and broader genomic characterizations of disease. Recently, the emergence of Big Data technologies has generated a lot of interest among the medical community concerned with breast health. Indeed, the available storage capacities increased exponentially during the last three decades, thus leading to bigger volumes and variety of stored medical data (e.g., mammography scans, 3D ultrasound, MRI, genomic data, pathological data, etcetera). However, Big data is not confined just to genomics. It includes DNA/RNA sequencing: Bulk, single cell, ChIPseq (CUT&RUN), ATACseq, spatial, Cell-free…; Proteomics, Screens (CRISPR/Cas9, RNAi, drug libraries), Digital pathology (H&E, highly multiplexed IF, image mass cytometry), Radiomics (Mammography, CT, MRI, PET (18FDG, 18FES, 18FTL, Na18F, etc),Clinical data (SEER, NCDB, claims data, EMR),-Real world data (CancerLinQ, ProjectGENIE, PCORI). [4]\n",
      "\n",
      "The Role of Big Data in Precision Medicine is increasingly gaining importance. Big Data often used for this considers individual variability in  genes, environment, and lifestyle. Several  multigene assays are available to estimate the risk for relapse  after definitive surgery.\n",
      "\n",
      "Examples include: [5]\n",
      "\n",
      " - MammaPrint (uses fresh/frozen tissues or formalin-fixed paraffin-embedded (FFPE) samples and examines 1,391 genes by microarray assay, and the results of 70 genes are used to classify patients into high- to low-risk for relapse)\n",
      " - Oncotype DX,(uses FFPE samples, is a multigene assay and examines 21 genes including five reference genes to estimate the risk of relapse at 10 years along with the benefit of adjuvant endocrine therapy and chemotherapy)\n",
      " - PAM50-based Prosigna (NanoString Technologies)  examines a 50-gene signature and estimates the risk of  recurrence (ROR)\n",
      " - EndoPredict (Myriad Genetics) is a clinically validated  multianalyte gene expression test which could predict the  ROR in patients with breast cancer at 10 years.\n",
      "The big promise of Big Data is to allow the exploitation of all data sources, including unstructured ones such as textual patients reports or images, thus influencing medical research, and ultimately patient care.\n",
      "\n",
      "A transition to novel therapies to treat these aggressive breast cancers amongst younger women is an expected output. Patient advocacy groups can play multiple roles to help maximize and leverage big data to better serve patients. \n",
      "\n",
      "With an aim to provide global data resources on Breast cancer (OMICS and Clinical with an initial focus on risk factors as a starting focus), OHSL has established a global Breast Cancer Data Alliance (BCDA). This consortium brings together clinical and non-clinical professionals / researchers from various locations in the globe to participate in this alliance and help identify key parameters which could explain current trends in BC in women.\n",
      "\n",
      "A Review titled Big data and Breast Cancer [7] from Northwestern Medical school set the foundation for the BCDA through a two pronged foray into the arena of Big Data and Breast Cancer with a breast cancer data catalog and an updated review to follow the previous one review.\n",
      "\n",
      "Accurate analysis through bioinformatic approaches supplemented by experimental verification of certain markers identified via a cross platform Data Catalog compiled from different regions of the world would be a key milestone towards better understanding and better treatment of cancer. A primary requirement for this is the creation of a Data Catalog.\n",
      "\n",
      "Now, OHSL has entered into a strategic partnership with Alation to use their technology to create a global data catalog on cancer to consolidate critical informational sources into a single metadata repository.\n",
      "\n",
      "The data catalog provides the users with a simple, searchable and unified resource that enables data discovery, no matter the sources’ original location. \n",
      "\n",
      "Motivation: The vast amount of breast cancer data and resources  prompted us to focus our search initially on OMICS and Clinical data resources with special focus on risk factors for Breast cancer data, global cohorts and repositories and related publications with associated data access modalities.\n",
      "\n",
      "Current: We are working on creating common meta tags to classify open datasets for breast cancers from different geographical regions while conducting comparative analytics and research to identify unique features for region specific breast cancers. There is a special focus on LMICs (Lower and Middle Income Countries) and breast cancer. In December 2022, the work of the Breast Cancer Data Alliance was presented at the 11th General Assembly and International Conference of Asian Pacific organisation of Cancer Prevention (APOCP, December 8 -10 2022 Kolkata India  https://apocp.info/?page_id=766)\n",
      "\n",
      "![One](downloaded_images/one.jpeg)\n",
      "![APOC Group Photo](downloaded_images/two.jpeg)\n",
      " \n",
      "\n",
      "Furthermore, cataloging Breast cancer resources with data on Primary prevention has been initiated by partnership with International Breast Cancer and Nutrition Consortium.This collaboration has been expanded currently into preparation for a Horizon EU Grant application under the category of Staying Healthy program, where the objectives are:\n",
      "\n",
      " - (i) to implement personalized preventive interventions in preadolescents with and without increased risk for breast cancer,\n",
      " - (ii) develop a multidata knowledge hub (kHUB) including an expanded multidata catalog to identify individual breast cancer risk factors and make evidence-based recommendations and\n",
      " - (iii) validate biomarkers of Oxidative stress OS-mediated epigenetic impact and breast cancer risk and measure the impact of the intervention in a preadolescent population.\n",
      "This conceptual approach is summarized in Figure 1 and 2 below; we have chosen breast cancer as case study to target a specific NCD that now occurs more in younger people than before and the risk of which is established during childhood .The ultimate aim is to develop a HeLP- Health literacy programme based on our KAP - Knowledge, Attitude and Practices; MuRCaPP-Multidata resource catalog for primary prevention; NED-Nutriepigenomics database; NuBI- Nutrition-based intervention design software; RAP- Risk Assessment and Prediction software and RisQ- Risk Information self-questionnaire.All of theses are interconnected as depicted in Fig 1 and 2. AI/ML/DL will be used as appropriate towards creation of the relevant softwares.\n",
      "\n",
      "![Figure 1](downloaded_images/SCR-20231129-cspl.png)\n",
      "![Figure 2](downloaded_images/SCR-20231129-ctaj.png)\n",
      "As part of this process, OHSL participated in the Phase 1 Think Tank Program, Institut de Cancérologie de l’Ouest, (15, rue André Boquel) Angers, France On-site and via videoconference November 16, 2023. As part of this event, OHSL co-chaired two discussion groups.\n",
      "\n",
      "![OHSL Co-chaired two discussions](downloaded_images/SCR-20231205-bprd.png)\n",
      "![EU Think Tank Working Group, November 2023](downloaded_images/three.jpeg)\n",
      "The primary goal of the OHSL BCDA is to provide global data resources on breast cancer, with an initial focus on risk factors. Key objectives include:\n",
      "\n",
      "Data Catalog Creation: Develop a comprehensive global data catalog on cancer using Alation's technology to consolidate critical informational sources into a single metadata repository.\n",
      "\n",
      "Data Classification and Analytics: Implement common metatags to classify open datasets for breast cancer from different regions and conduct comparative analytics to identify region-specific characteristics.\n",
      "\n",
      "Focus on LMICs: Place special emphasis on Lower and Middle Income Countries (LMICs) to address breast cancer disparities.\n",
      "\n",
      "Primary Prevention: Collaborate with the International Breast Cancer and Nutrition Consortium to catalog breast cancer resources related to primary prevention.\n",
      "\n",
      "The BCDA project involves collaboration with international researchers, healthcare professionals, patient advocacy groups, and organizations. Notably, our partnership with the International Breast Cancer and Nutrition Consortium extends into a Horizon EU Grant application, aiming to implement personalized preventive interventions and validate biomarkers.\n",
      "\n",
      " - Dr. Shruti Shukla (Co-ordinator, BCDA)\n",
      " - Prof. (Dr.) G. K. Rath (Former Chief of Cancer Institute, AIIMS, New Delhi & Head of NCI-INDIA)\n",
      " - Dr. Ravi Mehrotra (Honorary Chief Medical Officer, OHSL)\n",
      " - Dr. Susan E Clare (Northwestern University, Chicago)\n",
      " - Dr. Sophie A Lelièvre (Institut de Cancérologie de l’Ouest & Professor Emerita, Purdue)\n",
      " - Dr. Sulma I Mohammed (Professor, Cancer Biology, Purdue)\n",
      " - Dr. Mariana Bustamante Edurado (Northwestern University, Chicago)\n",
      " - Pamela L. Shaw (Northwestern University, Chicago)\n",
      " - Anil Srivastava (President, OHSL)\n",
      " - Cezary Mazurek, Dir. Poznan Supercomputing and Networking Center, PL\n",
      " - Mary Beth Terry (Professor, Department of Epidemiology - School of Public Health, Columbia University)\n",
      " - Sharon Ross (Program Director, Division of Cancer Prevention, National Cancer Institute, NIH, USA; member of the IBCN external advisory board)\n",
      " - Members of World Health Organization (WHO)\n",
      " - Alation\n",
      "[1] https://gco.iarc.fr/today/online-analysis-pie\n",
      "\n",
      "[2] DeSantis et al. Breast Cancer Res Treat.2019 Feb;173(3):743-747\n",
      "\n",
      "[3] Madhav MR, Nayagam SG, Biyani K, Pandey V, Kamal DG, Sabarimurugan S, Ramesh N, Gothandam KM, Jayaraj R. Epidemiologic analysis of breast cancer incidence, prevalence, and mortality in India: Protocol for a systematic review and meta-analyses. Medicine (Baltimore). 2018 Dec;97(52):e13680. doi: 10.1097/MD.0000000000013680. PMID: 30593138; PMCID: PMC6314759\n",
      "\n",
      "[4] Daniel Stover, MD (daniel.stover@osumc.edu) at SABCS 2021, https://www.sabcs.org/Program/Daily-Schedule/Day-1\n",
      "\n",
      "[5] Naito Y, Urasaki T. Precision medicine in breast cancer. Chin Clin Oncol. 2018 Jun;7(3):29. doi: 10.21037/cco.2018.06.04. PMID: 30056731.\n",
      "\n",
      "[6] Jourquin J, Reffey SB, Jernigan C, Levy M, Zinser G, Sabelko K, Pietenpol J, Sledge G Jr. Susan G. Komen Big Data for Breast Cancer Initiative: How Patient Advocacy Organizations Can Facilitate Using Big Data to Improve Patient Outcomes. JCO Precis Oncol. 2019 Sep 12;3:PO.19.00184. doi: 10.1200/PO.19.00184. PMID: 32923852\n",
      "\n",
      "[7] Clare, S., Shaw, P. “Big Data” for breast cancer: where to look and what you will find. npj Breast Cancer 2, 16031 (2016)\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "### BCDA Catalogue Preview\n",
      "\n",
      "![Catalogue Main View](downloaded_images/1.png)\n",
      "![Figure 2: BCDA Catalogue View 1](downloaded_images/2.png)\n",
      "![Figure 3: BCDA Catalogue View 2 (Data Tables)](downloaded_images/3.png)\n",
      "![Figure 4: Non-US Breast Cancer Data View](downloaded_images/4.png)\n",
      "![Figure 5: Non-US Breast Cancer Data Tables View ](downloaded_images/5.png)\n",
      "![Figure 6: Multi-Data Resource](downloaded_images/6.png)\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "\n",
    "download_folder = 'downloaded_images/'\n",
    "\n",
    "def save_image(image_url, folder):\n",
    "    try:\n",
    "        img_response = requests.get(image_url)\n",
    "        if img_response.status_code == 200:\n",
    "            img_name = os.path.basename(urlparse(image_url).path)\n",
    "            img_path = os.path.join(folder, img_name)\n",
    "            with open(img_path, 'wb') as img_file:\n",
    "                img_file.write(img_response.content)\n",
    "            return img_name\n",
    "        else:\n",
    "            print(f\"Failed to retrieve image. Status code: {img_response.status_code}\")\n",
    "            return None \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while saving image: {e}\")\n",
    "        return None\n",
    "\n",
    "def print_element(element, url, level=0):\n",
    "    indent = '    ' * level\n",
    "    if element.name == 'h1':\n",
    "        print(f\"\\n# {element.get_text()}\\n\")\n",
    "    elif element.name == 'h2':\n",
    "        print(f\"\\n## {element.get_text()}\\n\")\n",
    "    elif element.name == 'h3':\n",
    "        print(f\"\\n### {element.get_text()}\\n\")\n",
    "    elif element.name == 'p':\n",
    "        print(f\"{indent}{element.get_text()}\\n\")\n",
    "    elif element.name == 'ul':\n",
    "        for li in element.find_all('li'):\n",
    "            print(f\"{indent} - {li.get_text()}\")\n",
    "    elif element.name == 'img':\n",
    "        img_url = urljoin(url, element.get('src'))\n",
    "        img_name = save_image(img_url, download_folder)\n",
    "        if img_name:\n",
    "            img_path = os.path.join(download_folder, img_name)\n",
    "            print(f\"{indent}![{element.get('alt', 'Image')}]({img_path})\")\n",
    "\n",
    "def scrape_project(url, download_folder='downloaded_images'):\n",
    "    project_data = []\n",
    "    try:\n",
    "        if not os.path.exists(download_folder):\n",
    "            os.makedirs(download_folder)\n",
    "\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            nav = soup.find('nav')\n",
    "            if nav:\n",
    "                print(\"\\nNavigation Bar:\\n\")\n",
    "                for li in nav.find_all('li'):\n",
    "                    print(f\" - {li.get_text()}\")\n",
    "                print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "            main_content = soup.find('div', {'class': 'main-content'})\n",
    "            if not main_content:\n",
    "                main_content = soup.find('div', {'class': 'content'})\n",
    "\n",
    "            if main_content:\n",
    "                for element in main_content.find_all(['h1', 'h2', 'h3', 'p', 'ul', 'img', 'table']):\n",
    "                    if element.name == 'table':\n",
    "                        for row in element.find_all('tr'):\n",
    "                            cols = row.find_all('td')\n",
    "                            if len(cols) >= 2:\n",
    "                                first_td = cols[0].get_text(strip=True)\n",
    "                                second_td = cols[1].get_text(strip=True)\n",
    "                                project_data.append({\n",
    "                                    'type': 'table',\n",
    "                                    'first_td': first_td,\n",
    "                                    'second_td': second_td\n",
    "                                })\n",
    "                                print(f\"{first_td}: {second_td}\")\n",
    "                    else:\n",
    "                        project_data.append({\n",
    "                            'type': element.name,\n",
    "                            'text': element.get_text(strip=True),\n",
    "                            'url': url if element.name == 'img' else None\n",
    "                        })\n",
    "                        print_element(element, url)\n",
    "            else:\n",
    "                print(\"Main content not found. Please check the class name used to identify the main content.\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the website. Status code: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    return project_data\n",
    "\n",
    "project_url = input(\"Enter the URL of the project page: \").strip()\n",
    "\n",
    "if project_url.startswith('http://') or project_url.startswith('https://'):\n",
    "    data = scrape_project(project_url)\n",
    "    with open('projects_data.json', 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    # scrape_project(project_url)\n",
    "else:\n",
    "    print(\"Invalid URL. Please ensure the URL starts with http:// or https://\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Module langchain_community.document_loaders not found. Please install langchain-community to access this module. You can install it using `pip install -U langchain-community`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/_api/module_import.py:69\u001b[0m, in \u001b[0;36mcreate_importer.<locals>.import_by_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib64/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1004\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DirectoryLoader\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPTVectorStoreIndex\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the scraped data\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/document_loaders/__init__.py:378\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    377\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Look up attributes dynamically.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_import_attribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/_api/module_import.py:72\u001b[0m, in \u001b[0;36mcreate_importer.<locals>.import_by_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_module\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain_community\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m     73\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install langchain-community to access this module. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can install it using `pip install -U langchain-community`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: Module langchain_community.document_loaders not found. Please install langchain-community to access this module. You can install it using `pip install -U langchain-community`"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.indexes import GPTVectorStoreIndex\n",
    "\n",
    "# Load the scraped data\n",
    "with open('projects_data.json', 'r') as f:\n",
    "    projects_data = json.load(f)\n",
    "\n",
    "# Create an instance of LangChain's vector store index\n",
    "index = GPTVectorStoreIndex()\n",
    "\n",
    "# Add documents to the index\n",
    "for entry in projects_data:\n",
    "    if entry['type'] == 'table':\n",
    "        content = f\"{entry['first_td']}: {entry['second_td']}\"\n",
    "    else:\n",
    "        content = entry['text']\n",
    "    \n",
    "    index.add_document({\n",
    "        'title': entry['type'],\n",
    "        'content': content,\n",
    "        'metadata': {'url': entry.get('url', '')}\n",
    "    })\n",
    "\n",
    "print(\"Data indexed with LangChain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
